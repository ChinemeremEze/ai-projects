{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech-to-Text Converter\n",
    "\n",
    "\n",
    "<strong>Speech Recognition</strong> involves converting spoken language into text. This is typically done using a combination of signal processing and machine learning models.\n",
    "\n",
    "<h4>Key Steps:</h4>\n",
    "\n",
    "1. Feature Extraction: Extract features from the audio signal, such as Mel-Frequency Cepstral Coefficients (MFCCs).\n",
    "2. Modeling: Train a neural network (e.g., RNN, LSTM) to map these features to text.\n",
    "3. Decoding: Use a decoder to convert the output of the neural network into readable text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "# Load dataset (LibriSpeech, etc.)\n",
    "# Assume data is loaded and preprocessed into (features, labels) format\n",
    "characters = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', \n",
    "              'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', ' ']\n",
    "blank_token = len(characters)  # Assuming the blank token is the last index\n",
    "\n",
    "# Feature extraction (MFCCs)\n",
    "def extract_features(audio_file):\n",
    "    audio, sr = librosa.load(audio_file, sr=16000)\n",
    "    mfcc = librosa.feature.mfcc(audio, sr=sr, n_mfcc=13)\n",
    "    return mfcc.T\n",
    "\n",
    "# Function to encode text labels to integers\n",
    "def encode_text(text):\n",
    "    return [characters.index(c) for c in text]\n",
    "\n",
    "# For illustration, assuming the dataset is already loaded\n",
    "\n",
    "# Generate random MFCC features for 100 samples, each with a varying number of time steps\n",
    "x_train = [np.random.rand(np.random.randint(80, 120), 13) for _ in range(100)]\n",
    "# Generate random text labels for 100 samples\n",
    "text_labels = [\"hello\", \"world\", \"tensorflow\", \"speech\", \"recognition\", \"model\", \"example\"] * 14 + [\"end\"]\n",
    "y_train = [encode_text(label) for label in text_labels]\n",
    "\n",
    "\n",
    "input_length = np.array([x.shape[0] for x in x_train])\n",
    "label_length = np.array([len(y) for y in y_train])\n",
    "\n",
    "# Padding y_train to match the longest label length for training\n",
    "max_label_length = max(label_length)\n",
    "padded_y_train = np.zeros((len(y_train), max_label_length))\n",
    "for i, label in enumerate(y_train):\n",
    "    padded_y_train[i, :len(label)] = label\n",
    "\n",
    "\n",
    "# Model architecture for speech-to-text\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(None, 13)),  # 13 MFCC features\n",
    "    layers.LSTM(128, return_sequences=True),\n",
    "    layers.LSTM(128),\n",
    "    layers.Dense(len(characters), activation='softmax')  # len(characters) is the number of unique characters in text\n",
    "])\n",
    "\n",
    "# CTC Loss for speech recognition\n",
    "def ctc_loss(y_true, y_pred):\n",
    "    return tf.keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss=ctc_loss)\n",
    "\n",
    "# Train model\n",
    "model.fit(np.array(x_train, dtype=object), padded_y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Inference (Converting speech to text)\n",
    "# Function to decode predictions into text\n",
    "def decode_predictions(pred):\n",
    "    pred_indices = np.argmax(pred, axis=-1)\n",
    "    decoded_text = []\n",
    "    prev_index = blank_token  # Initialize with blank token\n",
    "    for index in pred_indices[0]:\n",
    "        if index != prev_index and index != blank_token:\n",
    "            decoded_text.append(characters[index])\n",
    "        prev_index = index\n",
    "    return ''.join(decoded_text)\n",
    "\n",
    "audio_file = 'assest/output.wav'\n",
    "mfcc = extract_features(audio_file)\n",
    "predictions = model.predict(np.expand_dims(mfcc, axis=0))\n",
    "text = decode_predictions(predictions)\n",
    "print(text)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
